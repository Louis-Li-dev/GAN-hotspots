{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install tqdm\n",
    "# %pip install python-dotenv\n",
    "# %pip install torch==2.4.0+cu118\n",
    "# %pip install scikit_learn==1.2.2\n",
    "# %pip install ipython\n",
    "# %pip install pandas\n",
    "# %pip install numpy\n",
    "# %pip install matplotlib\n",
    "# %pip install tabulate\n",
    "# %pip install scipy\n",
    "# %pip install git+https://github.com/Louis-Li-dev/ML_tool_kit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "parent_dir = os.path.join(os.getcwd(), '..')\n",
    "if parent_dir not in sys.path: sys.path.append(parent_dir)\n",
    "from utility.data_utils import *\n",
    "from utility.visuals import *\n",
    "from dotenv import load_dotenv\n",
    "from model.CNN import ConditionalSegmentationVAE\n",
    "from mkit.torch_support.tensor_utils import xy_to_tensordataset\n",
    "from torch import nn\n",
    "from IPython.display import clear_output\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "load_dotenv()\n",
    "DATA_DIR = os.getenv(\"DATA_DIR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(DATA_DIR): raise FileNotFoundError(\"Make sure the data directory is correctly placed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = get_files(DATA_DIR)\n",
    "\n",
    "return_list = []\n",
    "file = files[0]\n",
    "city_name = file.split('\\\\')[-1].split('.csv')[0].split('_')[0]\n",
    "\n",
    "path_name = process_and_transform_data(file, resolution=.5, overwrite=True)\n",
    "with open(path_name, 'rb') as f:\n",
    "    result_dict = pickle.load(f)\n",
    "labels = result_dict['labels']\n",
    "encoder = result_dict['encoder']\n",
    "MAX_LEN = result_dict['max length']\n",
    "file_name = result_dict['file name']\n",
    "WIDTH = result_dict['width']\n",
    "HEIGHT = result_dict['height']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for idx, label in enumerate(labels):\n",
    "#     plt.imshow(labels[idx])\n",
    "#     plt.savefig(f'../fig/{idx}_{file_name}.png')\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- x y splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "unique_labels = [u for u in labels if np.array(np.where(u != 0)).T.shape[0] > 1]\n",
    "padded_labels = []\n",
    "for label in unique_labels:\n",
    "    unique_vals = np.unique(label)[1:]\n",
    "    new_vals = []\n",
    "    count = 0\n",
    "    for val in unique_vals:    \n",
    "        dummy_vals = np.zeros(label.shape)\n",
    "        dummy_vals[np.where(label == val)] = 1\n",
    "        new_vals.append(dummy_vals)\n",
    "        count += 1\n",
    "    for i in range(count, MAX_LEN):\n",
    "        dummy_vals = np.zeros(label.shape)\n",
    "        new_vals.append(dummy_vals)\n",
    "    new_vals = np.array(new_vals)\n",
    "    padded_labels.append(new_vals)\n",
    "train_labels, test_labels = train_test_split(padded_labels, test_size=.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_channels, img_width, img_height, start_dim, n_layers, latent_dim):\n",
    "        \"\"\"\n",
    "        Fully Connected Encoder.\n",
    "        \n",
    "        Args:\n",
    "            input_channels (int): Number of input channels (e.g., 3 for RGB).\n",
    "            img_width (int): Image width.\n",
    "            img_height (int): Image height.\n",
    "            start_dim (int): Size of the first hidden layer.\n",
    "            n_layers (int): Number of hidden layers (each doubling the previous size).\n",
    "            latent_dim (int): Size of the latent representation.\n",
    "        \"\"\"\n",
    "        super(Encoder, self).__init__()\n",
    "        input_size = input_channels * img_width * img_height\n",
    "        layers = []\n",
    "        # First layer: from flattened input to start_dim\n",
    "        layers.append(nn.Linear(input_size, start_dim))\n",
    "        layers.append(nn.Mish())\n",
    "        hidden_dim = start_dim\n",
    "        # Add n_layers that double the hidden size at each layer\n",
    "        for _ in range(n_layers):\n",
    "            next_dim = hidden_dim * 2\n",
    "            layers.append(nn.Linear(hidden_dim, next_dim))\n",
    "            layers.append(nn.Mish())\n",
    "            hidden_dim = next_dim\n",
    "        # Final layer to produce latent representation\n",
    "        layers.append(nn.Linear(hidden_dim, latent_dim))\n",
    "        self.encoder = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Flatten input: (B, C, W, H) -> (B, C*W*H)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.encoder(x)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_channels, img_width, img_height, start_dim, n_layers, latent_dim):\n",
    "        \"\"\"\n",
    "        Fully Connected Decoder.\n",
    "        \n",
    "        Args:\n",
    "            output_channels (int): Number of output channels (should match input_channels).\n",
    "            img_width (int): Image width.\n",
    "            img_height (int): Image height.\n",
    "            start_dim (int): Should match the start_dim used in the encoder.\n",
    "            n_layers (int): Number of hidden layers (used in encoder, in reverse order here).\n",
    "            latent_dim (int): Size of the latent representation.\n",
    "        \"\"\"\n",
    "        super(Decoder, self).__init__()\n",
    "        output_size = output_channels * img_width * img_height\n",
    "        layers = []\n",
    "        # For symmetry, assume the encoderâ€™s last hidden dimension was start_dim * (2 ** n_layers)\n",
    "        hidden_dim = start_dim * (2 ** n_layers)\n",
    "        # First layer: from latent_dim to hidden_dim\n",
    "        layers.append(nn.Linear(latent_dim, hidden_dim))\n",
    "        layers.append(nn.Mish())\n",
    "        # Then, for each layer, halve the hidden dimension\n",
    "        for _ in range(n_layers):\n",
    "            next_dim = hidden_dim // 2\n",
    "            layers.append(nn.Linear(hidden_dim, next_dim))\n",
    "            layers.append(nn.Mish())\n",
    "            hidden_dim = next_dim\n",
    "        # Final layer: output layer to reconstruct the flattened image\n",
    "        layers.append(nn.Linear(hidden_dim, output_size))\n",
    "        layers.append(nn.Sigmoid())  # Ensures the output values are between 0 and 1\n",
    "        self.decoder = nn.Sequential(*layers)\n",
    "        self.output_channels = output_channels\n",
    "        self.img_width = img_width\n",
    "        self.img_height = img_height\n",
    "\n",
    "    def forward(self, z):\n",
    "        x = self.decoder(z)\n",
    "        # Reshape back to image dimensions: (B, output_channels, img_width, img_height)\n",
    "        x = x.view(x.size(0), self.output_channels, self.img_width, self.img_height)\n",
    "        return x\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_channels, img_width, img_height, start_dim, n_layers, latent_dim, output_channels):\n",
    "        \"\"\"\n",
    "        Fully Connected Autoencoder (combining the encoder and decoder).\n",
    "        \"\"\"\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = Encoder(input_channels, img_width, img_height, start_dim, n_layers, latent_dim)\n",
    "        self.decoder = Decoder(output_channels, img_width, img_height, start_dim, n_layers, latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "def generate(input_channels, img_width, img_height, start_dim, n_layers, latent_dim, output_channels, device=\"cpu\", output_type=\"autoencoder\"):\n",
    "    \"\"\"\n",
    "    Creates an encoder, decoder, or autoencoder model based on user input.\n",
    "\n",
    "    Args:\n",
    "        input_channels (int): Number of input channels (e.g., 3 for RGB).\n",
    "        img_width (int): Image width.\n",
    "        img_height (int): Image height.\n",
    "        start_dim (int): The size of the first hidden layer.\n",
    "        n_layers (int): Number of hidden layers (each doubling the dimension in the encoder).\n",
    "        latent_dim (int): Size of the latent representation.\n",
    "        output_channels (int): Number of output channels (should match input_channels).\n",
    "        device (str): 'cpu' or 'cuda'.\n",
    "        output_type (str): 'encoder', 'decoder', or 'autoencoder'.\n",
    "\n",
    "    Returns:\n",
    "        A PyTorch model on the selected device.\n",
    "    \"\"\"\n",
    "    device = torch.device(device)\n",
    "\n",
    "    if output_type == \"encoder\":\n",
    "        model = Encoder(input_channels, img_width, img_height, start_dim, n_layers, latent_dim)\n",
    "    elif output_type == \"decoder\":\n",
    "        model = Decoder(output_channels, img_width, img_height, start_dim, n_layers, latent_dim)\n",
    "    elif output_type == \"autoencoder\":\n",
    "        model = Autoencoder(input_channels, img_width, img_height, start_dim, n_layers, latent_dim, output_channels)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid output_type. Choose from 'encoder', 'decoder', or 'autoencoder'.\")\n",
    "    return model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loader, val_loader = xy_to_tensordataset(\n",
    "    train_labels, train_labels,\n",
    "    return_loader=True, \n",
    "    batch_size=8,\n",
    "    input_dtype=torch.float32,\n",
    "    output_dtype=torch.float32,\n",
    "    val_ratio=.15,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For Machine Learning Models\n",
    "    - To fit the data formats of tensors, every sci-kit learn model needs to be wrapped inside the object MLWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLWrapper(nn.Module):\n",
    "    def __init__(self, model_object = RandomForestRegressor, **args):\n",
    "        self.model = model_object(**args)\n",
    "        self.device = torch.device('cpu')\n",
    "    def loader_to_xy(self, loader):\n",
    "        x, y = loader.dataset.tensors\n",
    "        x, y = np.array(x).squeeze(1), np.array(y)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        return x, y\n",
    "    def fit(self, train_loader, val_loader):\n",
    "        train_x, train_y = self.loader_to_xy(train_loader)\n",
    "        val_train_x, val_train_y = self.loader_to_xy(val_loader)\n",
    "        self.model.fit(train_x, train_y)\n",
    "        accu = self.model.score(val_train_x, val_train_y)\n",
    "        print(accu)\n",
    "    def inference(self, img):\n",
    "        batch_size, _, _, _ = img.shape\n",
    "        img = img.reshape(batch_size, -1)\n",
    "        return torch.tensor(self.model.predict(img))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_CHANNELS = 1    # For grayscale images; use 3 for RGB.\n",
    "N_EPOCHS = 100      # Adjust as needed.\n",
    "LATENT_DIM = 300    # Dimensionality of the latent space.\n",
    "FEATURE_MAPS = 8    # Base number of feature maps.\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ConditionalSegmentationVAE(\n",
    "    latent_dim=LATENT_DIM,\n",
    "    width=WIDTH,\n",
    "    height=HEIGHT,\n",
    "    img_channels=IMG_CHANNELS,\n",
    "    feature_maps=FEATURE_MAPS,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "segmentation_loss_fn = nn.BCEWithLogitsLoss().to(device)\n",
    "\n",
    "# Assume your train_loader and val_loader are defined appropriately.\n",
    "model.train_vae(\n",
    "    train_loader=loader,       # your training DataLoader\n",
    "    val_loader=val_loader,       # your validation DataLoader\n",
    "    n_epochs=N_EPOCHS,\n",
    "    seg_criterion=segmentation_loss_fn,\n",
    "    kl_weight=0.001,\n",
    "    patience=10,\n",
    "    device=device\n",
    ")\n",
    "clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "evaluate_and_plot(test_loader, model=model, encoder=encoder, title='VAE', dataset_name=city_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model = MLWrapper()\n",
    "model.fit(loader, val_loader)\n",
    "evaluate_and_plot(test_loader, model=model, encoder=encoder, title='RF', dataset_name=city_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
