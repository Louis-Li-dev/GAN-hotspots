{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install tqdm\n",
    "# %pip install python-dotenv\n",
    "# %pip install torch==2.4.0+cu118\n",
    "# %pip install scikit_learn==1.2.2\n",
    "# %pip install ipython\n",
    "# %pip install pandas\n",
    "# %pip install numpy\n",
    "# %pip install matplotlib\n",
    "# %pip install tabulate\n",
    "# %pip install scipy\n",
    "# %pip install git+https://github.com/Louis-Li-dev/ML_tool_kit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "parent_dir = os.path.join(os.getcwd(), '..')\n",
    "if parent_dir not in sys.path: sys.path.append(parent_dir)\n",
    "from utility.data_utils import *\n",
    "from utility.visuals import *\n",
    "from dotenv import load_dotenv\n",
    "from model.CNN import ConditionalSegmentationVAE\n",
    "from mkit.torch_support.tensor_utils import xy_to_tensordataset\n",
    "from torch import nn\n",
    "from IPython.display import clear_output\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "load_dotenv()\n",
    "DATA_DIR = os.getenv(\"DATA_DIR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(DATA_DIR): raise FileNotFoundError(\"Make sure the data directory is correctly placed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original dataset size: 238\n",
      "dataset size with duplicates removed: 172\n"
     ]
    }
   ],
   "source": [
    "files = get_files(DATA_DIR)\n",
    "\n",
    "return_list = []\n",
    "file = files[0]\n",
    "city_name = file.split('\\\\')[-1].split('.csv')[0].split('_')[0]\n",
    "\n",
    "path_name = process_and_transform_data(file, resolution=.5, overwrite=True)\n",
    "with open(path_name, 'rb') as f:\n",
    "    result_dict = pickle.load(f)\n",
    "labels = result_dict['labels']\n",
    "encoder = result_dict['encoder']\n",
    "MAX_LEN = result_dict['max length']\n",
    "file_name = result_dict['file name']\n",
    "WIDTH = result_dict['width']\n",
    "HEIGHT = result_dict['height']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for idx, label in enumerate(labels):\n",
    "#     plt.imshow(labels[idx])\n",
    "#     plt.savefig(f'../fig/{idx}_{file_name}.png')\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- x y splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "unique_labels = [u for u in labels if np.array(np.where(u != 0)).T.shape[0] > 1]\n",
    "padded_labels = []\n",
    "for label in unique_labels:\n",
    "    unique_vals = np.unique(label)[1:]\n",
    "    new_vals = []\n",
    "    count = 0\n",
    "    for val in unique_vals:    \n",
    "        dummy_vals = np.zeros(label.shape)\n",
    "        dummy_vals[np.where(label == val)] = 1\n",
    "        new_vals.append(dummy_vals)\n",
    "        count += 1\n",
    "    for i in range(count, MAX_LEN):\n",
    "        dummy_vals = np.zeros(label.shape)\n",
    "        new_vals.append(dummy_vals)\n",
    "    new_vals = np.array(new_vals)\n",
    "    padded_labels.append(new_vals)\n",
    "train_labels, test_labels = train_test_split(unique_labels, test_size=.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=n_components)\n",
    "images_reduced = pca.fit_transform(images_flat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_channels, img_width, img_height, start_dim, n_layers, latent_dim):\n",
    "        \"\"\"\n",
    "        Fully Connected Encoder.\n",
    "        \n",
    "        Args:\n",
    "            input_channels (int): Number of input channels (e.g., 3 for RGB).\n",
    "            img_width (int): Image width.\n",
    "            img_height (int): Image height.\n",
    "            start_dim (int): Size of the first hidden layer.\n",
    "            n_layers (int): Number of hidden layers (each doubling the previous size).\n",
    "            latent_dim (int): Size of the latent representation.\n",
    "        \"\"\"\n",
    "        super(Encoder, self).__init__()\n",
    "        input_size = input_channels * img_width * img_height\n",
    "        layers = []\n",
    "        # First layer: from flattened input to start_dim\n",
    "        layers.append(nn.Linear(input_size, start_dim))\n",
    "        layers.append(nn.Mish())\n",
    "        hidden_dim = start_dim\n",
    "        # Add n_layers that double the hidden size at each layer\n",
    "        for _ in range(n_layers):\n",
    "            next_dim = hidden_dim * 2\n",
    "            layers.append(nn.Linear(hidden_dim, next_dim))\n",
    "            layers.append(nn.Mish())\n",
    "            hidden_dim = next_dim\n",
    "        # Final layer to produce latent representation\n",
    "        layers.append(nn.Linear(hidden_dim, latent_dim))\n",
    "        self.encoder = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Flatten input: (B, C, W, H) -> (B, C*W*H)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.encoder(x)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_channels, img_width, img_height, start_dim, n_layers, latent_dim):\n",
    "        \"\"\"\n",
    "        Fully Connected Decoder.\n",
    "        \n",
    "        Args:\n",
    "            output_channels (int): Number of output channels (should match input_channels).\n",
    "            img_width (int): Image width.\n",
    "            img_height (int): Image height.\n",
    "            start_dim (int): Should match the start_dim used in the encoder.\n",
    "            n_layers (int): Number of hidden layers (used in encoder, in reverse order here).\n",
    "            latent_dim (int): Size of the latent representation.\n",
    "        \"\"\"\n",
    "        super(Decoder, self).__init__()\n",
    "        output_size = output_channels * img_width * img_height\n",
    "        layers = []\n",
    "        # For symmetry, assume the encoderâ€™s last hidden dimension was start_dim * (2 ** n_layers)\n",
    "        hidden_dim = start_dim * (2 ** n_layers)\n",
    "        # First layer: from latent_dim to hidden_dim\n",
    "        layers.append(nn.Linear(latent_dim, hidden_dim))\n",
    "        layers.append(nn.Mish())\n",
    "        # Then, for each layer, halve the hidden dimension\n",
    "        for _ in range(n_layers):\n",
    "            next_dim = hidden_dim // 2\n",
    "            layers.append(nn.Linear(hidden_dim, next_dim))\n",
    "            layers.append(nn.Mish())\n",
    "            hidden_dim = next_dim\n",
    "        # Final layer: output layer to reconstruct the flattened image\n",
    "        layers.append(nn.Linear(hidden_dim, output_size))\n",
    "        self.decoder = nn.Sequential(*layers)\n",
    "        self.output_channels = output_channels\n",
    "        self.img_width = img_width\n",
    "        self.img_height = img_height\n",
    "\n",
    "    def forward(self, z):\n",
    "        x = self.decoder(z)\n",
    "        # Reshape back to image dimensions: (B, output_channels, img_width, img_height)\n",
    "        x = x.view(x.size(0), self.output_channels, self.img_width, self.img_height)\n",
    "        return x\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_channels, img_width, img_height, start_dim, n_layers, latent_dim, output_channels):\n",
    "        \"\"\"\n",
    "        Fully Connected Autoencoder (combining the encoder and decoder).\n",
    "        \"\"\"\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = Encoder(input_channels, img_width, img_height, start_dim, n_layers, latent_dim)\n",
    "        self.decoder = Decoder(output_channels, img_width, img_height, start_dim, n_layers, latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "def generate(input_channels, img_width, img_height, start_dim, n_layers, latent_dim, output_channels, device=\"cpu\", output_type=\"autoencoder\"):\n",
    "    \"\"\"\n",
    "    Creates an encoder, decoder, or autoencoder model based on user input.\n",
    "\n",
    "    Args:\n",
    "        input_channels (int): Number of input channels (e.g., 3 for RGB).\n",
    "        img_width (int): Image width.\n",
    "        img_height (int): Image height.\n",
    "        start_dim (int): The size of the first hidden layer.\n",
    "        n_layers (int): Number of hidden layers (each doubling the dimension in the encoder).\n",
    "        latent_dim (int): Size of the latent representation.\n",
    "        output_channels (int): Number of output channels (should match input_channels).\n",
    "        device (str): 'cpu' or 'cuda'.\n",
    "        output_type (str): 'encoder', 'decoder', or 'autoencoder'.\n",
    "\n",
    "    Returns:\n",
    "        A PyTorch model on the selected device.\n",
    "    \"\"\"\n",
    "    device = torch.device(device)\n",
    "\n",
    "    if output_type == \"encoder\":\n",
    "        model = Encoder(input_channels, img_width, img_height, start_dim, n_layers, latent_dim)\n",
    "    elif output_type == \"decoder\":\n",
    "        model = Decoder(output_channels, img_width, img_height, start_dim, n_layers, latent_dim)\n",
    "    elif output_type == \"autoencoder\":\n",
    "        model = Autoencoder(input_channels, img_width, img_height, start_dim, n_layers, latent_dim, output_channels)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid output_type. Choose from 'encoder', 'decoder', or 'autoencoder'.\")\n",
    "    return model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1360a222690>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x, y = next(iter(loader))\n",
    "plt.imshow(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loader, val_loader = xy_to_tensordataset(\n",
    "    train_labels, train_labels,\n",
    "    return_loader=True, \n",
    "    batch_size=8,\n",
    "    input_dtype=torch.float32,\n",
    "    output_dtype=torch.float32,\n",
    "    val_ratio=.15,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH 1/100:   0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ss348\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:101: UserWarning: Using a target size (torch.Size([8, 48, 64])) that is different to the input size (torch.Size([8, 1, 48, 64])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "EPOCH 1/100:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 8/15 [00:00<00:00, 72.28it/s]c:\\Users\\ss348\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:101: UserWarning: Using a target size (torch.Size([1, 48, 64])) that is different to the input size (torch.Size([1, 1, 48, 64])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "EPOCH 1/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:00<00:00, 63.20it/s]\n",
      "c:\\Users\\ss348\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:101: UserWarning: Using a target size (torch.Size([3, 48, 64])) that is different to the input size (torch.Size([3, 1, 48, 64])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100] Training Loss: 0.0357  Validation Loss: 0.0125\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH 2/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:00<00:00, 96.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/100] Training Loss: 0.0082  Validation Loss: 0.0051\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH 3/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:00<00:00, 105.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/100] Training Loss: 0.0042  Validation Loss: 0.0034\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH 4/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:00<00:00, 103.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/100] Training Loss: 0.0033  Validation Loss: 0.0030\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH 5/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:00<00:00, 102.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/100] Training Loss: 0.0030  Validation Loss: 0.0030\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH 6/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:00<00:00, 108.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/100] Training Loss: 0.0030  Validation Loss: 0.0028\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH 7/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:00<00:00, 89.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/100] Training Loss: 0.0029  Validation Loss: 0.0027\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH 8/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:00<00:00, 104.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/100] Training Loss: 0.0027  Validation Loss: 0.0028\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH 9/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:00<00:00, 98.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/100] Training Loss: 0.0027  Validation Loss: 0.0027\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH 10/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:00<00:00, 80.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100] Training Loss: 0.0028  Validation Loss: 0.0026\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH 11/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:00<00:00, 106.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/100] Training Loss: 0.0028  Validation Loss: 0.0027\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH 12/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:00<00:00, 93.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/100] Training Loss: 0.0028  Validation Loss: 0.0027\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH 13/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:00<00:00, 99.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/100] Training Loss: 0.0028  Validation Loss: 0.0026\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH 14/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:00<00:00, 109.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/100] Training Loss: 0.0028  Validation Loss: 0.0027\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH 15/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:00<00:00, 105.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/100] Training Loss: 0.0027  Validation Loss: 0.0026\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH 16/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:00<00:00, 94.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/100] Training Loss: 0.0027 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Validation Loss: 0.0026\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH 17/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:00<00:00, 102.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/100] Training Loss: 0.0027  Validation Loss: 0.0026\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH 18/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:00<00:00, 88.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/100] Training Loss: 0.0027  Validation Loss: 0.0027\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH 19/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:00<00:00, 104.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/100] Training Loss: 0.0027  Validation Loss: 0.0026\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH 20/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:00<00:00, 98.27it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/100] Training Loss: 0.0027  Validation Loss: 0.0027\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH 21/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:00<00:00, 102.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [21/100] Training Loss: 0.0028  Validation Loss: 0.0026\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH 22/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:00<00:00, 94.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [22/100] Training Loss: 0.0027  Validation Loss: 0.0027\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH 23/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:00<00:00, 93.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [23/100] Training Loss: 0.0027  Validation Loss: 0.0026\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH 24/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:00<00:00, 87.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [24/100] Training Loss: 0.0028  Validation Loss: 0.0026\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH 25/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:00<00:00, 100.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [25/100] Training Loss: 0.0028  Validation Loss: 0.0026\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH 26/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:00<00:00, 93.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [26/100] Training Loss: 0.0027  Validation Loss: 0.0026\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH 27/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:00<00:00, 85.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [27/100] Training Loss: 0.0027  Validation Loss: 0.0026\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH 28/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:00<00:00, 102.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [28/100] Training Loss: 0.0027  Validation Loss: 0.0026\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH 29/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:00<00:00, 99.69it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [29/100] Training Loss: 0.0028"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Validation Loss: 0.0026\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH 30/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:00<00:00, 92.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [30/100] Training Loss: 0.0027  Validation Loss: 0.0026\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH 31/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:00<00:00, 96.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [31/100] Training Loss: 0.0027  Validation Loss: 0.0026\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH 32/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:00<00:00, 96.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [32/100] Training Loss: 0.0028  Validation Loss: 0.0026\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH 33/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:00<00:00, 101.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [33/100] Training Loss: 0.0027  Validation Loss: 0.0026\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH 34/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:00<00:00, 105.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [34/100] Training Loss: 0.0028  Validation Loss: 0.0026\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH 35/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:00<00:00, 81.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [35/100] Training Loss: 0.0027  Validation Loss: 0.0026\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH 36/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:00<00:00, 100.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [36/100] Training Loss: 0.0027  Validation Loss: 0.0026\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH 37/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:00<00:00, 95.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [37/100] Training Loss: 0.0027  Validation Loss: 0.0026\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH 38/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:00<00:00, 110.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [38/100] Training Loss: 0.0028  Validation Loss: 0.0026\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH 39/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:00<00:00, 102.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [39/100] Training Loss: 0.0027  Validation Loss: 0.0026\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH 40/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:00<00:00, 95.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [40/100] Training Loss: 0.0027  Validation Loss: 0.0026\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH 41/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:00<00:00, 101.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [41/100] Training Loss: 0.0027  Validation Loss: 0.0026\n",
      "\n",
      "Training stopped after 6 consecutive iterations without improvement.\n",
      "Training completed.\n",
      "Best validation loss: 0.0025809326519568763, model loaded with the selected parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Autoencoder(\n",
       "  (encoder): Encoder(\n",
       "    (encoder): Sequential(\n",
       "      (0): Linear(in_features=3072, out_features=64, bias=True)\n",
       "      (1): Mish()\n",
       "      (2): Linear(in_features=64, out_features=128, bias=True)\n",
       "      (3): Mish()\n",
       "      (4): Linear(in_features=128, out_features=256, bias=True)\n",
       "      (5): Mish()\n",
       "      (6): Linear(in_features=256, out_features=12, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (decoder): Sequential(\n",
       "      (0): Linear(in_features=12, out_features=256, bias=True)\n",
       "      (1): Mish()\n",
       "      (2): Linear(in_features=256, out_features=128, bias=True)\n",
       "      (3): Mish()\n",
       "      (4): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (5): Mish()\n",
       "      (6): Linear(in_features=64, out_features=3072, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mkit.torch_support.nn_utils import training_loop\n",
    "model = Autoencoder(1, WIDTH, HEIGHT, start_dim=64, n_layers=2, latent_dim=12, output_channels=1)\n",
    "training_loop(\n",
    "    model, \n",
    "    train_loader=loader,\n",
    "    val_loader=val_loader,\n",
    "    optimizer=torch.optim.Adamax(model.parameters()),\n",
    "    criterion=nn.L1Loss(),\n",
    "    device=torch.device('cuda'),\n",
    "    epochs=100,\n",
    "    early_stopping=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For Machine Learning Models\n",
    "    - To fit the data formats of tensors, every sci-kit learn model needs to be wrapped inside the object MLWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model(torch.tensor(test_labels, dtype=torch.float32, device=torch.device('cuda')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48, 64)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "idx = 0\n",
    "pred_img, label_img  = pred.squeeze(1)[idx].cpu().detach(), test_labels[idx]\n",
    "# Create a figure with 1 row and 2 columns, and set an appropriate figure size\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Plot prediction image with a title and a shrunken colorbar with some padding\n",
    "im0 = axes[0].imshow(pred_img, cmap='viridis')\n",
    "axes[0].set_title(\"Prediction\")\n",
    "fig.colorbar(im0, ax=axes[0], shrink=0.7, pad=0.05)\n",
    "\n",
    "# Plot ground truth image with a title and a shrunken colorbar with some padding\n",
    "print(label_img.shape)\n",
    "im1 = axes[1].imshow(label_img, cmap='viridis')\n",
    "axes[1].set_title(\"Ground Truth\")\n",
    "fig.colorbar(im1, ax=axes[1], shrink=0.7, pad=0.05)\n",
    "\n",
    "# Adjust layout to leave space for the suptitle\n",
    "fig.subplots_adjust(top=0.85)\n",
    "NAME = f\"Autoencoder_{count}_L1\"\n",
    "fig.suptitle(NAME, fontsize=16)\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "PATH = f'../fig/AE_L1/{idx}'\n",
    "os.makedirs(PATH, exist_ok=True)\n",
    "\n",
    "# Save and close the figure\n",
    "plt.savefig(f\"{PATH}/{NAME}.png\")\n",
    "plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLWrapper(nn.Module):\n",
    "    def __init__(self, model_object = RandomForestRegressor, **args):\n",
    "        self.model = model_object(**args)\n",
    "        self.device = torch.device('cpu')\n",
    "    def loader_to_xy(self, loader):\n",
    "        x, y = loader.dataset.tensors\n",
    "        x, y = np.array(x).squeeze(1), np.array(y)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        return x, y\n",
    "    def fit(self, train_loader, val_loader):\n",
    "        train_x, train_y = self.loader_to_xy(train_loader)\n",
    "        val_train_x, val_train_y = self.loader_to_xy(val_loader)\n",
    "        self.model.fit(train_x, train_y)\n",
    "        accu = self.model.score(val_train_x, val_train_y)\n",
    "        print(accu)\n",
    "    def inference(self, img):\n",
    "        batch_size, _, _, _ = img.shape\n",
    "        img = img.reshape(batch_size, -1)\n",
    "        return torch.tensor(self.model.predict(img))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_CHANNELS = 1    # For grayscale images; use 3 for RGB.\n",
    "N_EPOCHS = 100      # Adjust as needed.\n",
    "LATENT_DIM = 300    # Dimensionality of the latent space.\n",
    "FEATURE_MAPS = 8    # Base number of feature maps.\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ConditionalSegmentationVAE(\n",
    "    latent_dim=LATENT_DIM,\n",
    "    width=WIDTH,\n",
    "    height=HEIGHT,\n",
    "    img_channels=IMG_CHANNELS,\n",
    "    feature_maps=FEATURE_MAPS,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "segmentation_loss_fn = nn.BCEWithLogitsLoss().to(device)\n",
    "\n",
    "# Assume your train_loader and val_loader are defined appropriately.\n",
    "model.train_vae(\n",
    "    train_loader=loader,       # your training DataLoader\n",
    "    val_loader=val_loader,       # your validation DataLoader\n",
    "    n_epochs=N_EPOCHS,\n",
    "    seg_criterion=segmentation_loss_fn,\n",
    "    kl_weight=0.001,\n",
    "    patience=10,\n",
    "    device=device\n",
    ")\n",
    "clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "evaluate_and_plot(test_loader, model=model, encoder=encoder, title='VAE', dataset_name=city_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model = MLWrapper()\n",
    "model.fit(loader, val_loader)\n",
    "evaluate_and_plot(test_loader, model=model, encoder=encoder, title='RF', dataset_name=city_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
